// ================================================================================================================================
// Milind Jagre
//
// Eminem Lyrics Analysis
// ================================================================================================================================

package co.milindjagre

import java.io.{ InputStream, IOException, PrintWriter }
import java.lang.Boolean
import java.sql.Timestamp
import java.text.{ DateFormat, SimpleDateFormat }
import java.time.{ ZoneId, ZonedDateTime }
import java.util.{ Calendar, Properties }
import org.apache.commons.io.IOUtils
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{ FileSystem, Path, FileUtil, FSDataOutputStream }
import org.apache.spark.rdd
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.{ SaveMode, SparkSession, Column, Row, Dataset, DataFrame }
import org.apache.spark.sql.functions.{ concat, lit, sha2, when, col }
import org.apache.spark.sql.types.{ StructType, StructField, StringType, IntegerType, DataType, FloatType, DoubleType, TimestampType }
import scala.io.Source
import scala.sys.process._
import scala.util.matching.Regex
import scala.util.Try
import java.lang.Long

object EminemLyricsAnalysis {

  def main(args: Array[String]): Unit = {

    // sample app_name used to create the SparkContext object
    val app_name = "EMINEM LYRICS ANALYSIS"

    // building the SparkSession object
    val spark = SparkSession
      .builder()
      .appName(app_name)
      .master("local[*]")
      .getOrCreate()

    // reading the input file from HDFS STAGING DIRECTORY
    val inputDF = spark
      .read
      .format("com.databricks.spark.csv")
      .option("header", "false")
      .option("delimiter", "\n")
      .load("/project/dz/collab/mjagre/spark/inputfile.txt")

    inputDF.show

    inputDF.foreach(
            row => row.toString().split(" ").foreach(
                    col => print("column:" + col + "\n")))

  }
}
